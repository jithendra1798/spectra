# SPECTRA

**Emotion-adaptive AI interface â€” your face controls the screen.**

You and an AI partner (ORACLE) collaborate through voice on a timed mission. Your webcam reads your emotions in real time, and the entire interface â€” layout, colors, complexity, voice tone â€” reshapes around your emotional state.

Built for the AI Interfaces Hackathon with Claude Â· Feb 21, 2026 Â· Betaworks NYC

---

## Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    emotion JSON     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   adaptation cmds   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Component A    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Component B    â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Component C    â”‚
â”‚ Emotion Pipeline â”‚     (every ~1s)     â”‚ AI Brain (ORACLE)â”‚    (per turn)       â”‚ Adaptive Frontendâ”‚
â”‚ Tavus Raven-1    â”‚                     â”‚ Claude API       â”‚                     â”‚ React + CopilotKitâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚            Component D                     â”‚
                          â”‚       Voice + Backend (FastAPI)            â”‚
                          â”‚    connects all pieces via WebSocket       â”‚
                          â”‚                    â”‚                       â”‚
                          â”‚              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                â”‚
                          â”‚              â”‚   Redis    â”‚                â”‚
                          â”‚              â”‚ state +    â”‚                â”‚
                          â”‚              â”‚ timeline   â”‚                â”‚
                          â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **Every component communicates through the JSON formats below. Build and test with mock data tonight â€” real integration tomorrow.**

---

## Data Contracts

These are the exact JSON shapes passed between components. If everyone respects these formats, integration will be smooth.

### A â†’ D: Emotion Signal
*Emotion Pipeline â†’ Backend (via WebSocket, every ~1 second)*

```json
{
  "timestamp": 1740153600000,
  "emotions": {
    "stress": 0.72,
    "focus": 0.15,
    "confusion": 0.58,
    "confidence": 0.10,
    "neutral": 0.05
  },
  "dominant": "stress",
  "face_detected": true
}
```
- All emotion values are `0-1` floats
- `dominant` = highest scoring emotion
- `face_detected` = `false` if no face in frame

---

### D â†’ B: ORACLE Prompt Context
*Backend â†’ AI Brain (per interaction turn)*

```json
{
  "game_state": {
    "phase": "vault",
    "time_remaining": 182,
    "decisions_made": 3,
    "current_score": 45
  },
  "emotion_snapshot": {
    "current": { },
    "trend": "rising_stress",
    "avg_stress_30s": 0.65
  },
  "player_input": "I think node A looks weakest",
  "conversation_history": [
    { "role": "oracle", "text": "Which entry point?" },
    { "role": "player", "text": "Node A looks weakest" }
  ]
}
```
- `phase`: `"infiltrate"` | `"vault"` | `"escape"`
- `trend`: computed from last 5 emotion readings
- `current`: latest full Emotion Signal JSON

---

### B â†’ D â†’ C: ORACLE Response + UI Commands
*AI Brain â†’ Backend â†’ Frontend (per interaction turn)*

```json
{
  "oracle_response": {
    "text": "Good instinct. Node A has the weakest encryption layer...",
    "voice_style": "calm_reassuring"
  },
  "ui_commands": {
    "complexity": "simplified",
    "color_mood": "calm",
    "panels_visible": ["main"],
    "options": [
      { "id": "A", "label": "Node A", "highlighted": true },
      { "id": "C", "label": "Node C", "highlighted": true }
    ],
    "guidance_level": "high"
  },
  "game_update": {
    "score_delta": 10,
    "advance_phase": false,
    "next_prompt": null
  }
}
```
- `voice_style`: `"calm_reassuring"` | `"direct_fast"` | `"urgent"` | `"neutral"`
- `complexity`: `"simplified"` | `"standard"` | `"full"`
- `color_mood`: `"calm"` | `"neutral"` | `"intense"`
- `guidance_level`: `"none"` | `"low"` | `"medium"` | `"high"`
- `next_prompt`: `null` if staying in current phase

---

### D â†’ Redis: Emotion Timeline Entry
*Backend â†’ Redis (stored every ~1 second, read by debrief screen)*

```json
{
  "t": 1740153600000,
  "phase": "vault",
  "stress": 0.72,
  "focus": 0.15,
  "adaptation": "ui_simplified"
}
```
- `adaptation`: `null` if no adaptation this tick, otherwise `"ui_simplified"` | `"voice_calmed"` | `"options_expanded"` | `"full_dashboard"`
- Stored as Redis sorted set, scored by timestamp
- Debrief screen reads the full set and renders the emotion curve + markers

---

## Component Instructions

Each component can be built and tested independently using `mock-data/`.

### Component A: Emotion Pipeline
**Tech:** Tavus Raven-1 Â· Webcam Â· WebSocket

**Goal:** Read the user's face via webcam, output Emotion Signal JSON every ~1 second.

**Build:**
- Webcam capture â€” request camera permission, get video stream
- Tavus Raven-1 API â€” send frames, receive emotion scores
- Parse into Emotion Signal JSON format
- Send to backend via WebSocket every ~1s
- Handle: no face detected, camera denied, API timeout

**Test independently:**
- Small test page: webcam + live emotion JSON printout
- Verify JSON matches contract exactly
- Try different expressions â€” does stress rise when you frown?

> **Fallback:** If Raven-1 gives trouble, build a manual emotion slider that outputs the same JSON. Rest of the system won't know the difference.

---

### Component B: AI Brain (ORACLE)
**Tech:** Claude API Â· Function Calling Â· Structured Output

**Goal:** Receive game state + emotion data, output ORACLE's response + UI adaptation commands.

**Build:**
- **System prompt** â€” ORACLE personality, mission context, emotion-to-UI mapping rules
- **Emotion profiling** â€” reason about raw scores to determine what user needs
- **Response generation** â€” given phase + emotion + input, generate text + UI commands
- **Structured output** â€” Claude must return exact ORACLE Response JSON every time
- **Scenario scripts** â€” one per phase, 2-3 decision points each

**Test independently:**
- Script that sends mock Context JSON to Claude, prints response
- Vary emotion snapshots â€” does high stress â†’ `"simplified"` commands?
- Verify JSON is parseable and matches contract

> **The system prompt is the most important deliverable.** ORACLE's quality = prompt quality.

---

### Component C: Adaptive Frontend
**Tech:** React Â· CopilotKit Â· CSS Transitions

**Goal:** Render mission UI. Dynamically swap layout, colors, components based on UI commands.

**Build:**
- **3 phase screens** â€” Infiltrate (node grid), Vault (code puzzle), Escape (route map)
- **Adaptive rendering** â€” simplified / standard / full variants per `ui_commands.complexity`
- **Color mood system** â€” CSS variables shift per `ui_commands.color_mood`
- **Panel visibility** â€” show/hide per `ui_commands.panels_visible`
- **Option rendering** â€” dynamic from `ui_commands.options` array
- **Timer** â€” countdown display, color shifts as time drops
- **Debrief screen** â€” timeline from Redis, stress curve + adaptation markers
- **CopilotKit** â€” generative UI for dynamic component rendering

**Test independently:**
- Mock panel with buttons: "stressed" / "focused" / "advance phase"
- Feed mock ORACLE Response JSON, verify UI changes visually
- Test debrief with mock timeline array

> **Priority:** One phase working with adaptive rendering first â†’ duplicate â†’ debrief last (it's the demo closer, make it polished).

---

### Component D: Voice I/O + Backend + Integration
**Tech:** FastAPI Â· WebSocket Â· Web Speech API Â· Redis Â· ElevenLabs (optional)

**Goal:** Route data between components. Voice in/out. Game state. Emotion timeline storage.

**Build:**
- **FastAPI + WebSocket** â€” receive emotion data from A, route to B
- **Game state manager** â€” phase, timer, score, conversation history
- **Voice input** â€” Web Speech API (browser) â†’ text â†’ backend
- **Voice output** â€” TTS for ORACLE (browser TTS or ElevenLabs)
- **Redis** â€” store timeline entries + game state, expose API for debrief
- **Orchestration loop** â€” emotion + voice â†’ context JSON â†’ Claude â†’ response â†’ frontend + speak
- **Timer** â€” server-side 5-min countdown, broadcast to frontend

**Test independently:**
- FastAPI running, send mock emotion via WebSocket, verify routing
- Voice input â†’ text arrives at server
- Redis write/read timeline entries
- Full mock loop end-to-end

> **Integration is the biggest risk.** Get WebSocket routing working first with dummy data. Once data flows, everything else is refinement.

---

## Repo Structure

```
spectra/
â”œâ”€â”€ emotion-pipeline/        # Component A
â”‚   â”œâ”€â”€ tavus_client.py
â”‚   â”œâ”€â”€ webcam.js
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ oracle-brain/            # Component B
â”‚   â”œâ”€â”€ system_prompt.txt
â”‚   â”œâ”€â”€ scenarios.py
â”‚   â”œâ”€â”€ claude_client.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ frontend/                # Component C
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ phases/
â”‚   â”‚   â”œâ”€â”€ debrief/
â”‚   â”‚   â”œâ”€â”€ adaptive/
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ backend/                 # Component D
â”‚   â”œâ”€â”€ server.py
â”‚   â”œâ”€â”€ game_state.py
â”‚   â”œâ”€â”€ redis_client.py
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ mock-data/               # Shared mock data
â”‚   â”œâ”€â”€ emotion_signal.json
â”‚   â”œâ”€â”€ oracle_response.json
â”‚   â”œâ”€â”€ context_payload.json
â”‚   â””â”€â”€ timeline_entries.json
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

> **mock-data/** is key â€” everyone tests against the same JSONs. If your component produces correct output from mock input, integration will work.

---

## Tonight's Checklist

**Emotion Pipeline**
- [ ] Webcam capture working
- [ ] Tavus API call working (or fallback slider)
- [ ] Outputting correct Emotion Signal JSON

**AI Brain**
- [ ] System prompt written and tested
- [ ] Claude returning valid ORACLE Response JSON
- [ ] Infiltrate phase scenario working

**Adaptive Frontend**
- [ ] One phase with simplified / standard / full variants
- [ ] Color mood switching on mock input
- [ ] Debrief screen rendering mock timeline

**Voice + Backend**
- [ ] FastAPI + WebSocket running
- [ ] Voice â†’ text working
- [ ] Redis storing/retrieving timeline entries
- [ ] Mock data flowing through full loop

---

## Setup

```bash
cp .env.example .env
# Fill in your API keys
```

See each component's README for specific instructions.

---

**Let's build.** ðŸš€
